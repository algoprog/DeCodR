{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Data Processing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_VoQOkZgGevW",
        "outputId": "70afba23-7e42-4f3f-8468-50643bfa8d44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import json\n",
        "\n",
        "from collections import defaultdict\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from bs4 import BeautifulSoup"
      ],
      "metadata": {
        "id": "JSbpK3n9G1-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_csv(path: str, row_names: List[str]) -> List[Dict[str, Any]]:\n",
        "  data = []\n",
        "  with open(path) as f:\n",
        "    reader = csv.reader(f)\n",
        "    for row in reader:\n",
        "      entry = {}\n",
        "      for name, value in zip(row_names, row):\n",
        "        entry[name] = value\n",
        "      data.append(entry)\n",
        "  return data\n",
        "\n",
        "def merge_question_answers(questions: List[Dict[str, Any]], answers: List[Dict[str, Any]]):\n",
        "  answers_for_question = defaultdict(list)\n",
        "  for answer in answers:\n",
        "    answers_for_question[answer['questionId']].append(answer)\n",
        "\n",
        "  merged = []\n",
        "  for question in questions:\n",
        "    merged.append({\n",
        "        'question': question,\n",
        "        'answers': answers_for_question[question['id']]\n",
        "    })\n",
        "  return merged\n",
        "\n",
        "def save_json(data, path):\n",
        "  with open(path, 'w') as f:\n",
        "    json.dump(data, f)\n",
        "\n",
        "def answers_to_collection(answer_path, collection_path):\n",
        "  answer_rows = ['id', 'questionId', 'body', 'score']\n",
        "  answers = sorted([[answer['id'], answer['body']] for answer in read_csv(answer_path, answer_rows)])\n",
        "  with open(collection_path, 'w') as f:\n",
        "    writer = csv.writer(f, delimiter='\\t')\n",
        "    for answer in answers:\n",
        "      writer.writerow(answer)\n",
        "\n",
        "def answers_to_qrels(answer_path, qrels_path):\n",
        "  answer_rows = ['id', 'questionId', 'body', 'score']\n",
        "  answers = sorted([[answer['id'], answer['questionId']] for answer in read_csv(answer_path, answer_rows)])\n",
        "  qrels = []\n",
        "  for answer in answers:\n",
        "    qrels.append([answer[1] + '_t', 0, answer[0], 1])\n",
        "    qrels.append([answer[1] + '_q', 0, answer[0], 1])\n",
        "\n",
        "  with open(collection_path, 'w') as f:\n",
        "    writer = csv.writer(f, delimiter='\\t')\n",
        "    for qrel in qrels:\n",
        "      writer.writerow(qrel)\n",
        "\n",
        "def questions_to_queries(question_path, query_path):\n",
        "  question_rows = ['id', 'title', 'tags', 'body', 'acceptedAnswerId', 'score', 'views']\n",
        "  raw_questions = read_csv(question_path, question_rows)\n",
        "  questions = []\n",
        "\n",
        "  for question in raw_questions:\n",
        "    questions.append([question['id'] + '_t', question['title']])\n",
        "    questions.append([question['id'] + '_q', question['body']])\n",
        "\n",
        "  with open(query_path, 'w') as f:\n",
        "    writer = csv.writer(f, delimiter='\\t')\n",
        "    for question in questions:\n",
        "      writer.writerow(question)"
      ],
      "metadata": {
        "id": "MDdGYZJkG-zF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls drive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TeOIEgdpHk8R",
        "outputId": "c5fc8736-5dae-4402-d0e4-4c8431281413"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MyDrive  Shareddrives\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "answer_path = 'drive/Shared drives/685 Final Project/Stackoverflow Data/large_answers.csv'\n",
        "collection_path = 'drive/Shared drives/685 Final Project/Stackoverflow Data/Training Data/collection.tsv'\n",
        "answers_to_collection(answer_path, collection_path)"
      ],
      "metadata": {
        "id": "xQigRt-jam3Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qrels_path = 'drive/Shared drives/685 Final Project/Stackoverflow Data/Training Data/qrels.tsv'\n",
        "answers_to_qrels(answer_path, qrels_path)"
      ],
      "metadata": {
        "id": "-huCb_wOgfru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question_path = 'drive/Shared drives/685 Final Project/Stackoverflow Data/large_questions.csv'\n",
        "query_path = 'drive/Shared drives/685 Final Project/Stackoverflow Data/Training Data/queries.tsv'\n",
        "questions_to_queries(question_path, query_path)"
      ],
      "metadata": {
        "id": "lEJ4bJvEebrz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def creating_clean_questions(questions_path = 'drive/Shared drives/685 Final Project/Stackoverflow Data/questions_small.csv',answers_path = 'drive/Shared drives/685 Final Project/Stackoverflow Data/small_answers.csv'):\n",
        "\n",
        "  question_rows = ['id', 'title', 'tags', 'body', 'acceptedAnswerId', 'score', 'views']\n",
        "  # questions = pd.read_csv(questions_path, question_rows)\n",
        "  questions = pd.read_csv(questions_path,names = question_rows)\n",
        "\n",
        "  questions[\"body\"] = questions[['body']].applymap(lambda text : BeautifulSoup(text).get_text())\n",
        "\n",
        "  answer_rows = ['id', 'questionId', 'title', 'body', 'score', 'views']\n",
        "  # answers = pd.read_csv(answers_path, answer_rows)\n",
        "  answers = pd.read_csv(answers_path,names = answer_rows)\n",
        "\n",
        "  answers[\"body\"] = answers[['body']].applymap(lambda text : BeautifulSoup(text).get_text())\n",
        "  return questions, answers"
      ],
      "metadata": {
        "id": "MHVw6k58HYJZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_data = merge_question_answers(questions, answers)\n",
        "save_json(merged_data, 'drive/Shared drives/685 Final Project/Stackoverflow Data/small_merged.json')"
      ],
      "metadata": {
        "id": "MIrX1RhoK4AB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def train_valid_test_por(questions,test_por = 0.2,valid_por = 0.1,state = 2):\n",
        "\n",
        "  questions_train_val, questions_test = train_test_split(questions,test_size = test_por,random_state = state)\n",
        "  questions_train, questions_valid = train_test_split(questions_train_val,test_size = valid_por/(1-test_por),random_state = state)\n",
        "  return questions_train, questions_valid, questions_test\n",
        "\n",
        "def train_valid_test_fix(questions,test_val = 50000,valid_val = 50000,state = 2):\n",
        "  full_size = questions.size\n",
        "  test_por = test_val/full_size\n",
        "  valid_por = valid_val/full_size\n",
        "  questions_train_val, questions_test = train_test_split(questions,test_size = test_por,random_state = state)\n",
        "  questions_train, questions_valid = train_test_split(questions_train_val,test_size = valid_por/(1-test_por),random_state = state)\n",
        "  return questions_train, questions_valid, questions_test\n",
        "\n",
        "def create_qfiles(questions,path):\n",
        "  q_train,q_valid,q_test = train_valid_test_fix(questions)\n",
        "  q_train.to_csv(path + '/questions_train_sid.csv')\n",
        "  q_valid.to_csv(path + '/questions_valid_sid.csv')\n",
        "  q_test.to_csv(path + '/questions_test_sid.csv')\n",
        "\n",
        "\n",
        "ques,ans = creating_clean_questions()\n",
        "test_path = 'drive/Shared drives/685 Final Project/Stackoverflow Data'\n",
        "create_qfiles(ques,test_path)\n",
        "\n",
        "# help = {}\n",
        "# answers_test = pd.DataFrame().reindex_like(answers)\n",
        "# for q in questions_test:\n",
        "#   help[q[\"questionID\"]] = 1\n",
        "\n",
        "# for p in answers:\n",
        "#   if help.has_key[p[\"questionID\"]]:\n",
        "#     answers_test.append(p,ignore_index = True)"
      ],
      "metadata": {
        "id": "l-kqwLjAyynK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "OyOgMy9F9UIs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}